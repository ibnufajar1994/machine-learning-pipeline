{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Pipeline for Machine Learning Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build ETL Data Pipeline for Integrated Machine Learning Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data engineer is a crucial role in ecosystem of data, they responsible to provide precise and accurate data for implementation of machine learning model developed by data scientist. The responsibelity of data engineer including:\n",
    "\n",
    "#### 1. **Data Pipeline Development**\n",
    "   - Design and implement **ETL/ELT pipelines** (Extract, Transform, Load).  \n",
    "   - Automate workflows using tools like **Apache Airflow, Luigi, or NiFi**.  \n",
    "   - Ensure reliable and efficient data movement from sources to storage.\n",
    "\n",
    "#### 2. **Data Storage & Management**\n",
    "   - Manage **SQL & NoSQL databases** (PostgreSQL, MySQL, MongoDB, Cassandra).  \n",
    "   - Work with **data warehouses** (Snowflake, BigQuery, Redshift) and **data lakes** (Hadoop, Delta Lake, S3).  \n",
    "   - Optimize storage for performance and cost efficiency.\n",
    "\n",
    "#### 3. **Data Processing & Transformation**\n",
    "   - Process data in **batch (Spark, Hadoop)** and **real-time (Kafka, Flink)**.  \n",
    "   - Clean, normalize, and transform raw data for analytics.  \n",
    "   - Implement data quality checks and validation.  \n",
    "\n",
    "\n",
    "#### 4. **Data Governance & Security**\n",
    "   - Enforce **data security** (encryption, access controls).  \n",
    "   - Ensure compliance with **GDPR, CCPA, HIPAA**.  \n",
    "   - Maintain **metadata management & data lineage**.  \n",
    "\n",
    "On this project, we will learn to build integrated data pipeline of machine learning model. We will not focus on developing good quality data to feed the model instead of develop the machine learning model. \n",
    "\n",
    "#### A. Dataset\n",
    "- Run the Docker compose from this [repository](https://github.com/ibnufajar1994/machine-learning-pipeline)\n",
    "- This dataset compiles detailed car sales records from diverse sources, including databases, APIs, and spreadsheets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STAGING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data From Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from src.utils.helper import auth_gspread\n",
    "from src.utils.engine import init_engine\n",
    "from src.utils.load_log import LOAD_LOG\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GS_KEY = os.getenv(\"GS_KEY\")\n",
    "CRED_PATH = os.getenv(\"CRED_PATH\")\n",
    "worksheet_name = os.getenv(\"WORKSHEET_NAME\")\n",
    "link_api = os.getenv(\"LINK_API\")\n",
    "\n",
    "def extract_spreadsheet():\n",
    "    try:\n",
    "\n",
    "        gc = auth_gspread()\n",
    "            \n",
    "        # init spreadsheet by key\n",
    "        sheet_result = gc.open_by_key(GS_KEY)\n",
    "                \n",
    "        # read spreadsheet data\n",
    "        worksheet_result = sheet_result.worksheet(worksheet_name)\n",
    "\n",
    "        # convert it to dataframe\n",
    "        df_result = pd.DataFrame(worksheet_result.get_all_values())\n",
    "                \n",
    "        # set first rows as headers columns\n",
    "        df_result.columns = df_result.iloc[0]\n",
    "                \n",
    "        # get all the rest of the values\n",
    "        df_result = df_result[1:].copy()\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"component\":\"extraction\",\n",
    "            \"status\": \"success!\",\n",
    "            \"table_name\": \"brand_car\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "        }\n",
    "        \n",
    "        return df_result\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"component\":\"extraction\",\n",
    "            \"status\": \"failed!\",\n",
    "            \"table_name\": \"brand_car\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        LOAD_LOG(log_msg)\n",
    "        \n",
    "def extract_db_source(table_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "\n",
    "        src_engine = init_engine(\"source\")\n",
    "\n",
    "        df_data = pd.read_sql(sql = f\"select * from {table_name}\",\n",
    "                              con = src_engine)\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"component\":\"extraction\",\n",
    "            \"status\": \"success!\",\n",
    "            \"table_name\": \"car_sales\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "        }\n",
    "\n",
    "        return df_data\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"component\":\"extraction\",\n",
    "            \"status\": \"failed\",\n",
    "            \"table_name\": \"car_sales\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        LOAD_LOG(log_msg)\n",
    "\n",
    "\n",
    "\n",
    "def extract_api():\n",
    "    try:\n",
    "        response = requests.get(link_api)\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Jika data memiliki kunci 'regions', gunakan itu\n",
    "        if 'regions' in data:\n",
    "            df = pd.DataFrame(data['regions'])\n",
    "        else:\n",
    "            # Jika tidak, gunakan seluruh data\n",
    "            df = pd.DataFrame(data)\n",
    "        \n",
    "        log_msg = {\n",
    "            \"step\": \"staging\",\n",
    "            \"component\": \"extraction\",\n",
    "            \"status\": \"success!\",\n",
    "            \"table_name\": \"us_state\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\": \"staging\",\n",
    "            \"component\": \"extraction\",\n",
    "            \"status\": \"failed\",\n",
    "            \"table_name\": \"us_state\",\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        LOAD_LOG(log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sales</th>\n",
       "      <th>year</th>\n",
       "      <th>brand_car</th>\n",
       "      <th>model</th>\n",
       "      <th>trim</th>\n",
       "      <th>body</th>\n",
       "      <th>transmission</th>\n",
       "      <th>vin</th>\n",
       "      <th>state</th>\n",
       "      <th>condition</th>\n",
       "      <th>odometer</th>\n",
       "      <th>color</th>\n",
       "      <th>interior</th>\n",
       "      <th>seller</th>\n",
       "      <th>mmr</th>\n",
       "      <th>sellingprice</th>\n",
       "      <th>saledate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Impala Limited</td>\n",
       "      <td>LT Fleet</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>automatic</td>\n",
       "      <td>2g1wb5e37e1112559</td>\n",
       "      <td>fl</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21507.0</td>\n",
       "      <td>white</td>\n",
       "      <td>black</td>\n",
       "      <td>gm remarketing</td>\n",
       "      <td>13450.0</td>\n",
       "      <td>13800.0</td>\n",
       "      <td>Mon Feb 23 2015 05:00:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2003</td>\n",
       "      <td>Dodge</td>\n",
       "      <td>Ram Pickup 1500</td>\n",
       "      <td>SLT</td>\n",
       "      <td>Quad Cab</td>\n",
       "      <td></td>\n",
       "      <td>1d7ha18n13s152972</td>\n",
       "      <td>mo</td>\n",
       "      <td>31.0</td>\n",
       "      <td>79712.0</td>\n",
       "      <td>—</td>\n",
       "      <td>black</td>\n",
       "      <td>tdaf remarketing</td>\n",
       "      <td>6025.0</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>Tue Jan 20 2015 02:30:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>Pontiac</td>\n",
       "      <td>G6</td>\n",
       "      <td>GT</td>\n",
       "      <td>Convertible</td>\n",
       "      <td>automatic</td>\n",
       "      <td>1g2zh361474252178</td>\n",
       "      <td>nj</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65698.0</td>\n",
       "      <td>red</td>\n",
       "      <td>black</td>\n",
       "      <td>car authority inc</td>\n",
       "      <td>7375.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>Wed Jan 14 2015 01:30:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Corolla</td>\n",
       "      <td>LE</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>automatic</td>\n",
       "      <td>jtdbu4eexb9167571</td>\n",
       "      <td>fl</td>\n",
       "      <td>43.0</td>\n",
       "      <td>23634.0</td>\n",
       "      <td>black</td>\n",
       "      <td>beige</td>\n",
       "      <td>world omni financial corporation</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>11400.0</td>\n",
       "      <td>Tue Jan 27 2015 01:30:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2012</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>ES 350</td>\n",
       "      <td>Base</td>\n",
       "      <td>Sedan</td>\n",
       "      <td></td>\n",
       "      <td>jthbk1eg6c2495519</td>\n",
       "      <td>pa</td>\n",
       "      <td>35.0</td>\n",
       "      <td>26483.0</td>\n",
       "      <td>black</td>\n",
       "      <td>brown</td>\n",
       "      <td>meridian remarketing</td>\n",
       "      <td>22500.0</td>\n",
       "      <td>23300.0</td>\n",
       "      <td>Fri Jan 30 2015 01:00:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29996</td>\n",
       "      <td>2012</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Explorer</td>\n",
       "      <td>XLT</td>\n",
       "      <td>SUV</td>\n",
       "      <td>automatic</td>\n",
       "      <td>1fmhk8d83cga41998</td>\n",
       "      <td>mi</td>\n",
       "      <td>36.0</td>\n",
       "      <td>103016.0</td>\n",
       "      <td>white</td>\n",
       "      <td>gray</td>\n",
       "      <td>automobiles paille inc</td>\n",
       "      <td>17700.0</td>\n",
       "      <td>16800.0</td>\n",
       "      <td>Thu Jan 29 2015 01:30:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29997</td>\n",
       "      <td>2012</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Jetta</td>\n",
       "      <td>Base</td>\n",
       "      <td>sedan</td>\n",
       "      <td>automatic</td>\n",
       "      <td>3vw2k7aj1cm312846</td>\n",
       "      <td>fl</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41092.0</td>\n",
       "      <td>—</td>\n",
       "      <td>tan</td>\n",
       "      <td>vw credit</td>\n",
       "      <td>8475.0</td>\n",
       "      <td>9800.0</td>\n",
       "      <td>Wed Jun 10 2015 02:40:00 GMT-0700 (PDT)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29998</td>\n",
       "      <td>2003</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Tacoma</td>\n",
       "      <td>Base</td>\n",
       "      <td>Regular Cab</td>\n",
       "      <td></td>\n",
       "      <td>5tenl42n03z286594</td>\n",
       "      <td>az</td>\n",
       "      <td>19.0</td>\n",
       "      <td>292925.0</td>\n",
       "      <td>white</td>\n",
       "      <td>gray</td>\n",
       "      <td>ge fleet services for itself/servicer</td>\n",
       "      <td>3225.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>Thu May 21 2015 05:00:00 GMT-0700 (PDT)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29999</td>\n",
       "      <td>2014</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Impala Limited</td>\n",
       "      <td>LT Fleet</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>automatic</td>\n",
       "      <td>2g1wb5e37e1147702</td>\n",
       "      <td>fl</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25083.0</td>\n",
       "      <td>gray</td>\n",
       "      <td>gray</td>\n",
       "      <td>gm remarketing</td>\n",
       "      <td>12900.0</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>Mon Jan 26 2015 05:00:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>30000</td>\n",
       "      <td>2012</td>\n",
       "      <td>RAM</td>\n",
       "      <td>1500</td>\n",
       "      <td>Laramie Longhorn Edition</td>\n",
       "      <td>Crew Cab</td>\n",
       "      <td></td>\n",
       "      <td>1c6rd7pt4cs303383</td>\n",
       "      <td>pa</td>\n",
       "      <td>41.0</td>\n",
       "      <td>45750.0</td>\n",
       "      <td>black</td>\n",
       "      <td>black</td>\n",
       "      <td>j magnone auto group llc</td>\n",
       "      <td>29700.0</td>\n",
       "      <td>31700.0</td>\n",
       "      <td>Fri Jan 30 2015 01:30:00 GMT-0800 (PST)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_sales  year   brand_car            model                      trim  \\\n",
       "0             1  2014   Chevrolet   Impala Limited                  LT Fleet   \n",
       "1             2  2003       Dodge  Ram Pickup 1500                       SLT   \n",
       "2             3  2007     Pontiac               G6                        GT   \n",
       "3             4  2011      Toyota          Corolla                        LE   \n",
       "4             5  2012       Lexus           ES 350                      Base   \n",
       "...         ...   ...         ...              ...                       ...   \n",
       "29995     29996  2012        Ford         Explorer                       XLT   \n",
       "29996     29997  2012  Volkswagen            Jetta                      Base   \n",
       "29997     29998  2003      Toyota           Tacoma                      Base   \n",
       "29998     29999  2014   Chevrolet   Impala Limited                  LT Fleet   \n",
       "29999     30000  2012         RAM             1500  Laramie Longhorn Edition   \n",
       "\n",
       "              body transmission                vin state  condition  odometer  \\\n",
       "0            Sedan    automatic  2g1wb5e37e1112559    fl        4.0   21507.0   \n",
       "1         Quad Cab               1d7ha18n13s152972    mo       31.0   79712.0   \n",
       "2      Convertible    automatic  1g2zh361474252178    nj       34.0   65698.0   \n",
       "3            Sedan    automatic  jtdbu4eexb9167571    fl       43.0   23634.0   \n",
       "4            Sedan               jthbk1eg6c2495519    pa       35.0   26483.0   \n",
       "...            ...          ...                ...   ...        ...       ...   \n",
       "29995          SUV    automatic  1fmhk8d83cga41998    mi       36.0  103016.0   \n",
       "29996        sedan    automatic  3vw2k7aj1cm312846    fl       36.0   41092.0   \n",
       "29997  Regular Cab               5tenl42n03z286594    az       19.0  292925.0   \n",
       "29998        Sedan    automatic  2g1wb5e37e1147702    fl        4.0   25083.0   \n",
       "29999     Crew Cab               1c6rd7pt4cs303383    pa       41.0   45750.0   \n",
       "\n",
       "       color interior                                 seller      mmr  \\\n",
       "0      white    black                         gm remarketing  13450.0   \n",
       "1          —    black                       tdaf remarketing   6025.0   \n",
       "2        red    black                      car authority inc   7375.0   \n",
       "3      black    beige       world omni financial corporation  10800.0   \n",
       "4      black    brown                   meridian remarketing  22500.0   \n",
       "...      ...      ...                                    ...      ...   \n",
       "29995  white     gray                 automobiles paille inc  17700.0   \n",
       "29996      —      tan                              vw credit   8475.0   \n",
       "29997  white     gray  ge fleet services for itself/servicer   3225.0   \n",
       "29998   gray     gray                         gm remarketing  12900.0   \n",
       "29999  black    black               j magnone auto group llc  29700.0   \n",
       "\n",
       "       sellingprice                                 saledate  \n",
       "0           13800.0  Mon Feb 23 2015 05:00:00 GMT-0800 (PST)  \n",
       "1            6300.0  Tue Jan 20 2015 02:30:00 GMT-0800 (PST)  \n",
       "2            8000.0  Wed Jan 14 2015 01:30:00 GMT-0800 (PST)  \n",
       "3           11400.0  Tue Jan 27 2015 01:30:00 GMT-0800 (PST)  \n",
       "4           23300.0  Fri Jan 30 2015 01:00:00 GMT-0800 (PST)  \n",
       "...             ...                                      ...  \n",
       "29995       16800.0  Thu Jan 29 2015 01:30:00 GMT-0800 (PST)  \n",
       "29996        9800.0  Wed Jun 10 2015 02:40:00 GMT-0700 (PDT)  \n",
       "29997        2400.0  Thu May 21 2015 05:00:00 GMT-0700 (PDT)  \n",
       "29998       12800.0  Mon Jan 26 2015 05:00:00 GMT-0800 (PST)  \n",
       "29999       31700.0  Fri Jan 30 2015 01:30:00 GMT-0800 (PST)  \n",
       "\n",
       "[30000 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_car_sales = extract_db_source(\"car_sales\")\n",
    "df_car_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_car_id</th>\n",
       "      <th>brand_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Acura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Audi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bentley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BMW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Buick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 brand_car_id brand_name\n",
       "1            1      Acura\n",
       "2            2       Audi\n",
       "3            3    Bentley\n",
       "4            4        BMW\n",
       "5            5      Buick"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_car_brand = extract_spreadsheet()\n",
    "df_car_brand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_state</th>\n",
       "      <th>code</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>al</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ak</td>\n",
       "      <td>Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>az</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ar</td>\n",
       "      <td>Arkansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ca</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_state code        name\n",
       "0         1   al     Alabama\n",
       "1         2   ak      Alaska\n",
       "2         3   az     Arizona\n",
       "3         4   ar    Arkansas\n",
       "4         5   ca  California"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_state = extract_api()\n",
    "df_us_state.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data to Staging Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils.load_log import LOAD_LOG\n",
    "from src.utils.engine import init_engine\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def load_to_staging(data: pd.DataFrame, table_name: str) -> None:\n",
    "    try:\n",
    "        stg_engine = init_engine(\"staging\")\n",
    "\n",
    "        data = data.copy()\n",
    "\n",
    "        data.to_sql(name = table_name,\n",
    "                        con = stg_engine,\n",
    "                        if_exists = \"replace\",\n",
    "                        index = False)\n",
    "        \n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"component\":\"load\",\n",
    "            \"status\": \"success!\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "        }\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"component\":\"load\",\n",
    "            \"status\": \"failed!\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        LOAD_LOG(log_msg)\n",
    "        stg_engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_to_staging(data=df_car_sales, table_name=\"car_sales\")\n",
    "load_to_staging(data=df_car_brand, table_name=\"car_brand\")\n",
    "load_to_staging(data=df_us_state, table_name=\"us_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **WAREHOUSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data From Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils.engine import init_engine\n",
    "from src.utils.load_log import LOAD_LOG\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "def extract_db_staging(table_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "\n",
    "        src_engine = init_engine(\"staging\")\n",
    "\n",
    "        df_data = pd.read_sql(sql = f\"select * from {table_name}\",\n",
    "                              con = src_engine)\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"component\":\"extraction\",\n",
    "            \"status\": \"success!\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "        }\n",
    "\n",
    "        return df_data\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        log_msg = {\n",
    "            \"step\" : \"staging\",\n",
    "            \"component\":\"extraction\",\n",
    "            \"status\": \"failed\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        LOAD_LOG(log_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = extract_db_staging(\"car_sales\") #df 1 is car_sales\n",
    "df2 = extract_db_staging(\"car_brand\") #df 1 is car_brand\n",
    "df3 = extract_db_staging(\"us_state\")#df 3 is us_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Data after Extracted from Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.load_log import LOAD_LOG\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "class Transformation():\n",
    "\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, table_name: str) -> None:\n",
    "        self.data = data\n",
    "        self.table_name = table_name\n",
    "        self.columns = data.columns\n",
    "\n",
    "    \n",
    "    def drop_missing_value(self, col_name):\n",
    "        try:\n",
    "            if isinstance(col_name, str):\n",
    "                col_name = [col_name]\n",
    "            \n",
    "            self.data = self.data.dropna(subset=col_name)\n",
    "\n",
    "            # Update column information\n",
    "            self.columns = self.data.columns\n",
    "\n",
    "                    \n",
    "            log_msg = {\n",
    "                    \"step\" : \"Warehouse\",\n",
    "                    \"component\":\"Transformation (Drop Missing Value)\",\n",
    "                    \"status\": \"success!\",\n",
    "                    \"table_name\": self.table_name,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "                \n",
    "\n",
    "            return self.data\n",
    "        \n",
    "        except Exception as e:\n",
    "            \n",
    "            log_msg = {\n",
    "                    \"step\" : \"Warehouse\",\n",
    "                    \"component\":\"Transformation (Drop Missing Value)\",\n",
    "                    \"status\": \"Failed!\",\n",
    "                    \"table_name\": self.table_name,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "                    \"error_msg\": str(e)\n",
    "                }\n",
    "\n",
    "            \n",
    "        finally:\n",
    "            LOAD_LOG(log_msg)\n",
    "\n",
    "    \n",
    "    def drop_invalid_value(self, col_names: list, invalid_values: list):\n",
    "        try:\n",
    "\n",
    "            # Iterasi setiap kolom yang ada dalam col_names\n",
    "            for col_name in col_names:\n",
    "                # Hapus baris yang memiliki nilai invalid pada kolom ini\n",
    "                self.data = self.data[~self.data[col_name].isin(invalid_values)]\n",
    "\n",
    "            # Update column information\n",
    "            self.columns = self.data.columns\n",
    "\n",
    "            # Logging message\n",
    "            log_msg = {\n",
    "                    \"step\" : \"Warehouse\",\n",
    "                    \"component\": \"Transformation (Drop Invalid Value)\",\n",
    "                    \"status\": \"success!\",\n",
    "                    \"table_name\": self.table_name,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "\n",
    "            return self.data\n",
    "\n",
    "        except Exception as e:\n",
    "            # Logging error message\n",
    "            log_msg = {\n",
    "                    \"step\" : \"Warehouse\",\n",
    "                    \"component\": \"Transformation (Drop Invalid Value)\",\n",
    "                    \"status\": \"Failed!\",\n",
    "                    \"table_name\": self.table_name,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "                    \"error_msg\": str(e)\n",
    "                }\n",
    "\n",
    "        finally:\n",
    "            # Ensure that log is written regardless of success or failure\n",
    "            LOAD_LOG(log_msg)\n",
    "\n",
    "\n",
    "    def to_lower_case(self, col_names: list):\n",
    "        try:\n",
    "\n",
    "            # Ubah semua nilai di kolom yang ditentukan menjadi huruf kapital\n",
    "            for col_name in col_names:\n",
    "                self.data[col_name] = self.data[col_name].str.lower()\n",
    "\n",
    "            # Update column information\n",
    "            self.columns = self.data.columns\n",
    "\n",
    "            # Logging message\n",
    "            log_msg = {\n",
    "                    \"step\" : \"Warehouse\",\n",
    "                    \"component\": \"Transformation (To Lower Case)\",\n",
    "                    \"status\": \"success!\",\n",
    "                    \"table_name\": self.table_name,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "                }\n",
    "\n",
    "            return self.data\n",
    "\n",
    "        except Exception as e:\n",
    "            # Logging error message\n",
    "            log_msg = {\n",
    "                    \"step\" : \"Warehouse\",\n",
    "                    \"component\": \"Transformation (To Lower Case)\",\n",
    "                    \"status\": \"Failed!\",\n",
    "                    \"table_name\": self.table_name,\n",
    "                    \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "                    \"error_msg\": str(e)\n",
    "                }\n",
    "\n",
    "        finally:\n",
    "            # Ensure that log is written regardless of success or failure\n",
    "            LOAD_LOG(log_msg)\n",
    "\n",
    "\n",
    "    def join_data(self, df1: pd.DataFrame, df2: pd.DataFrame):\n",
    "        try:\n",
    "            # Simpan dataframe asli (self.data) sebagai df\n",
    "            df = self.data\n",
    "            \n",
    "            # Join df dengan df1 berdasarkan brand_car dan brand_name\n",
    "            merged_df = df.merge(\n",
    "                df1,\n",
    "                left_on=\"brand_car\",\n",
    "                right_on=\"brand_name\",\n",
    "                how=\"left\"\n",
    "            )\n",
    "            \n",
    "            # Join hasil merge pertama dengan df2 berdasarkan state dan code\n",
    "            final_df = merged_df.merge(\n",
    "                df2,\n",
    "                left_on=\"state\",\n",
    "                right_on=\"code\",\n",
    "                how=\"left\"\n",
    "            )\n",
    "            \n",
    "            # Update dataframe internal\n",
    "            self.data = final_df\n",
    "            \n",
    "            # Update column information\n",
    "            self.columns = self.data.columns\n",
    "            \n",
    "            # Logging message\n",
    "            log_msg = {\n",
    "                \"step\": \"Warehouse\",\n",
    "                \"component\": \"Transformation (Join Data)\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": self.table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Current timestamp\n",
    "            }\n",
    "            \n",
    "            return self.data\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Logging error message\n",
    "            log_msg = {\n",
    "                \"step\": \"Warehouse\",\n",
    "                \"component\": \"Transformation (Join Data)\",\n",
    "                \"status\": \"Failed!\",\n",
    "                \"table_name\": self.table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "            \n",
    "        finally:\n",
    "            # Ensure that log is written regardless of success or failure\n",
    "            LOAD_LOG(log_msg)\n",
    "\n",
    "\n",
    "    def select_merged_columns(self):\n",
    "        try:\n",
    "            # Daftar kolom yang dipilih (TANPA brand_car!)\n",
    "            selected_columns = [\n",
    "                \"id_sales\", \"year\", \"brand_car_id\", \"transmission\", \"id_state\", \"odometer\", \n",
    "                \"condition\", \"color\", \"interior\", \"mmr\", \"sellingprice\"\n",
    "            ]\n",
    "            \n",
    "            # Cek kolom yang tersedia\n",
    "            for col in selected_columns:\n",
    "                if col not in self.data.columns:\n",
    "                    print(f\"Warning: Column '{col}' not found in dataframe\")\n",
    "            \n",
    "            # Filter hanya kolom yang benar-benar ada\n",
    "            valid_columns = [col for col in selected_columns if col in self.data.columns]\n",
    "            \n",
    "            # Memperbarui self.data secara internal\n",
    "            self.data = self.data[valid_columns]\n",
    "            \n",
    "            # Update column information\n",
    "            self.columns = self.data.columns\n",
    "            \n",
    "            \n",
    "            log_msg = {\n",
    "                \"step\": \"Warehouse\",\n",
    "                \"component\": \"Transformation (Select Merged Columns)\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": self.table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            \n",
    "            return self.data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in select_merged_columns: {str(e)}\")\n",
    "            log_msg = {\n",
    "                \"step\": \"Warehouse\",\n",
    "                \"component\": \"Transformation (Select Merged Columns)\",\n",
    "                \"status\": \"Failed!\",\n",
    "                \"table_name\": self.table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "            \n",
    "        finally:\n",
    "            LOAD_LOG(log_msg)\n",
    "\n",
    "    def cast_columns(self):\n",
    "        try:\n",
    "            # Dictionary mapping tipe data string ke tipe data pandas\n",
    "            type_mapping = {\n",
    "                \"integer\": \"int64\",\n",
    "                \"float\": \"float64\",\n",
    "                \"string\": \"object\"\n",
    "            }\n",
    "            \n",
    "            # Dictionary kolom dan tipe data target\n",
    "            data_types = {\n",
    "                \"id_sales\": \"integer\",\n",
    "                \"year\": \"integer\",\n",
    "                \"brand_car_id\": \"integer\",\n",
    "                \"transmission\": \"string\",\n",
    "                \"id_state\": \"integer\",\n",
    "                \"condition\": \"float\", \n",
    "                \"odometer\": \"float\",\n",
    "                \"color\": \"string\",\n",
    "                \"interior\": \"string\",\n",
    "                \"mmr\": \"float\",\n",
    "                \"sellingprice\": \"float\"\n",
    "            }\n",
    "            \n",
    "            # Seleksi hanya kolom yang ada di dataframe\n",
    "            existing_columns = [col for col in data_types.keys() if col in self.data.columns]\n",
    "            \n",
    "            # Lakukan casting untuk setiap kolom\n",
    "            for column in existing_columns:\n",
    "                try:\n",
    "                    target_type = type_mapping[data_types[column]]\n",
    "                    \n",
    "                    # Tangani nilai yang mungkin error saat konversi\n",
    "                    if data_types[column] in [\"integer\", \"float\"]:\n",
    "                        # Konversi ke numerik, dengan coercing errors menjadi NaN\n",
    "                        self.data[column] = pd.to_numeric(self.data[column], errors='coerce')\n",
    "                    \n",
    "                    # Terapkan tipe data\n",
    "                    self.data[column] = self.data[column].astype(target_type)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error casting column {column}: {str(e)}\")\n",
    "            \n",
    "            # Logging message\n",
    "            log_msg = {\n",
    "                \"step\": \"Warehouse\",\n",
    "                \"component\": \"Transformation (Cast Columns)\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": self.table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            \n",
    "            return self.data\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Logging error message\n",
    "            log_msg = {\n",
    "                \"step\": \"Warehouse\",\n",
    "                \"component\": \"Transformation (Cast Columns)\",\n",
    "                \"status\": \"Failed!\",\n",
    "                \"table_name\": self.table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "            \n",
    "        finally:\n",
    "            # Ensure that log is written regardless of success or failure\n",
    "            LOAD_LOG(log_msg)\n",
    "\n",
    "    def rename_columns(self):\n",
    "        try:\n",
    "            # Dictionary mapping nama kolom original ke nama kolom baru\n",
    "            column_mapping = {\n",
    "                \"id_sales\": \"id_sales_nk\",\n",
    "                \"sellingprice\": \"selling_price\"\n",
    "            }\n",
    "            \n",
    "            # Filter hanya kolom yang ada di dataframe\n",
    "            valid_columns = {k: v for k, v in column_mapping.items() if k in self.data.columns}\n",
    "            \n",
    "            # Rename kolom\n",
    "            if valid_columns:\n",
    "                self.data = self.data.rename(columns=valid_columns)\n",
    "                \n",
    "                # Update column information\n",
    "                self.columns = self.data.columns\n",
    "            \n",
    "            # Logging message\n",
    "            log_msg = {\n",
    "                \"step\": \"Warehouse\",\n",
    "                \"component\": \"Transformation (Rename Columns)\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": self.table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            \n",
    "            return self.data\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Logging error message\n",
    "            log_msg = {\n",
    "                \"step\": \"Warehouse\",\n",
    "                \"component\": \"Transformation (Rename Columns)\",\n",
    "                \"status\": \"Failed!\",\n",
    "                \"table_name\": self.table_name,\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "            \n",
    "        finally:\n",
    "            # Ensure that log is written regardless of success or failure\n",
    "            LOAD_LOG(log_msg)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_trans = Transformation(data=df1, table_name=\"car_sales\")\n",
    "df2_trans = Transformation(data =df2, table_name=\"car_brand\")\n",
    "df3_trans = Transformation(data=df3, table_name=\"us_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformation of df1 (car_sales data)\n",
    "missing_value_col = [\"odometer\", \"mmr\", \"condition\"]\n",
    "invalid_value_col = [\"brand_car\", \"model\", \"trim\", \"body\", \"transmission\", \"vin\", \"state\", \"color\", \"interior\", \"seller\"]\n",
    "invalid_value = [\"\", \"—\",\"3vwd17aj5fm219943\", \"3vwd17aj5fm297123\"]\n",
    "to_lowercase_col = [\"brand_car\", \"model\", \"trim\", \"body\", \"transmission\", \"color\", \"interior\", \"seller\"]\n",
    "\n",
    "df1_clean = df1_trans.drop_missing_value(missing_value_col)\n",
    "df1_clean = df1_trans.drop_invalid_value(invalid_value_col, invalid_value)\n",
    "df1_clean = df1_trans.to_lower_case(to_lowercase_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformation of df2 (car_brand data)\n",
    "to_lowercase_col2 = [\"brand_name\"]\n",
    "df2_clean = df2_trans.to_lower_case(to_lowercase_col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformation of df3 (us_state data)\n",
    "to_lowercase_col3 = [\"name\"]\n",
    "df3_clean = df3_trans.to_lower_case(to_lowercase_col3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_clean = df1_trans.join_data(df2_clean, df3_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sales_nk</th>\n",
       "      <th>year</th>\n",
       "      <th>brand_car_id</th>\n",
       "      <th>transmission</th>\n",
       "      <th>id_state</th>\n",
       "      <th>odometer</th>\n",
       "      <th>condition</th>\n",
       "      <th>color</th>\n",
       "      <th>interior</th>\n",
       "      <th>mmr</th>\n",
       "      <th>selling_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>automatic</td>\n",
       "      <td>9</td>\n",
       "      <td>21507.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>white</td>\n",
       "      <td>black</td>\n",
       "      <td>13450.0</td>\n",
       "      <td>13800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>38</td>\n",
       "      <td>automatic</td>\n",
       "      <td>30</td>\n",
       "      <td>65698.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>red</td>\n",
       "      <td>black</td>\n",
       "      <td>7375.0</td>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2011</td>\n",
       "      <td>48</td>\n",
       "      <td>automatic</td>\n",
       "      <td>9</td>\n",
       "      <td>23634.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>black</td>\n",
       "      <td>beige</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>11400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2012</td>\n",
       "      <td>17</td>\n",
       "      <td>automatic</td>\n",
       "      <td>38</td>\n",
       "      <td>29050.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>burgundy</td>\n",
       "      <td>tan</td>\n",
       "      <td>12350.0</td>\n",
       "      <td>12700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2012</td>\n",
       "      <td>13</td>\n",
       "      <td>automatic</td>\n",
       "      <td>13</td>\n",
       "      <td>39151.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>red</td>\n",
       "      <td>tan</td>\n",
       "      <td>9525.0</td>\n",
       "      <td>9700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_sales_nk  year  brand_car_id transmission  id_state  odometer  \\\n",
       "0            1  2014             7    automatic         9   21507.0   \n",
       "1            3  2007            38    automatic        30   65698.0   \n",
       "2            4  2011            48    automatic         9   23634.0   \n",
       "3            6  2012            17    automatic        38   29050.0   \n",
       "4            7  2012            13    automatic        13   39151.0   \n",
       "\n",
       "   condition     color interior      mmr  selling_price  \n",
       "0        4.0     white    black  13450.0        13800.0  \n",
       "1       34.0       red    black   7375.0         8000.0  \n",
       "2       43.0     black    beige  10800.0        11400.0  \n",
       "3       34.0  burgundy      tan  12350.0        12700.0  \n",
       "4       32.0       red      tan   9525.0         9700.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sel [15]\n",
    "df1_clean = df1_trans.select_merged_columns()\n",
    "\n",
    "\n",
    "# Sel [16]\n",
    "df1_clean = df1_trans.cast_columns()\n",
    "\n",
    "# Sel [17]\n",
    "df1_clean = df1_trans.rename_columns()\n",
    "\n",
    "df1_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data into Warehouse Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils.load_log import LOAD_LOG\n",
    "from src.utils.engine import init_engine\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def load_to_warehouse(data: pd.DataFrame, table_name: str) -> None:\n",
    "    try:\n",
    "        stg_engine = init_engine(\"warehouse\")\n",
    "\n",
    "        data = data.copy()\n",
    "\n",
    "        data.to_sql(name = table_name,\n",
    "                        con = stg_engine,\n",
    "                        if_exists = \"append\",\n",
    "                        index = False)\n",
    "        \n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"component\":\"load\",\n",
    "            \"status\": \"success!\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "        }\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        log_msg = {\n",
    "            \"step\" : \"warehouse\",\n",
    "            \"component\":\"load\",\n",
    "            \"status\": \"failed!\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),  # Current timestamp\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        LOAD_LOG(log_msg)\n",
    "        stg_engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_to_warehouse(data=df1_clean, table_name=\"car_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MACHINE LEARNING MODEL**\n",
    "\n",
    "#### Extract Clean Data from Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils.engine import init_engine\n",
    "from src.utils.load_log import LOAD_LOG\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_warehouse(table_name: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        # Initialize database engine\n",
    "        src_engine = init_engine(\"warehouse\")\n",
    "\n",
    "        # Extract data from SQL table\n",
    "        df_data = pd.read_sql(sql=f\"select * from {table_name}\",\n",
    "                              con=src_engine)\n",
    "\n",
    "        # Exclude the last column ('created_at')\n",
    "        df_data = df_data.iloc[:, :-1]  # Or use df_data.drop(columns=[\"created_at\"])\n",
    "\n",
    "        # Log success message\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"extraction\",\n",
    "            \"status\": \"success!\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "\n",
    "        return df_data\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log failure message in case of an error\n",
    "        log_msg = {\n",
    "            \"step\": \"modelling\",\n",
    "            \"component\": \"extraction\",\n",
    "            \"status\": \"failed\",\n",
    "            \"table_name\": table_name,\n",
    "            \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error_msg\": str(e)\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        # Save the log\n",
    "        LOAD_LOG(log_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_warehouse(table_name=\"car_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import os\n",
    "from minio import Minio\n",
    "from src.utils.load_log import LOAD_LOG\n",
    "\n",
    "class CarPriceModel:\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the car price prediction model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            DataFrame containing car data\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.features = ['odometer_log', 'condition', 'car_age', 'brand_car_id', 'transmission', 'color', 'mmr']\n",
    "        self.target = 'selling_price'\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.X_encoded = None\n",
    "        self.y = None\n",
    "        \n",
    "    def feature_engineering(self):\n",
    "        \"\"\"\n",
    "        Create new features needed for the model\n",
    "        \"\"\"       \n",
    "        try:\n",
    "\n",
    "\n",
    "            # Calculate the car age based on the reference year 2015\n",
    "            self.data['car_age'] = 2015 - self.data['year']\n",
    "            \n",
    "            # Log transformation for the odometer\n",
    "            self.data['odometer_log'] = np.log1p(self.data['odometer'])\n",
    "\n",
    "            # Log success message\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Feature Engineering\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "\n",
    "            \n",
    "            return self\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log failure message in case of an error\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Feature Engineering\",\n",
    "                \"status\": \"failed\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            # Save the log\n",
    "            LOAD_LOG(log_msg)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare data for modeling, including encoding categorical features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # One-hot encoding for categorical variables\n",
    "            self.X_encoded = pd.get_dummies(self.data[self.features], drop_first=True)\n",
    "            \n",
    "            # Target is 'selling_price'\n",
    "            self.y = self.data[self.target]\n",
    "            # Log success message\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Prepare Data\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "            \n",
    "            return self\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log failure message in case of an error\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Prepare Data\",\n",
    "                \"status\": \"failed\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            # Save the log\n",
    "            LOAD_LOG(log_msg)\n",
    "    \n",
    "    def scale_features(self):\n",
    "        \"\"\"\n",
    "        Normalize features using StandardScaler\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            self.X_scaled = self.scaler.fit_transform(self.X_encoded)\n",
    "\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Scale Features\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "\n",
    "            return self\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log failure message in case of an error\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Scale Feature\",\n",
    "                \"status\": \"failed\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            # Save the log\n",
    "            LOAD_LOG(log_msg)\n",
    "    \n",
    "    def split_data(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Split data into training and testing sets\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to be used for testing\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                self.X_scaled, self.y, test_size=test_size, random_state=random_state\n",
    "            )\n",
    "\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Splitting Data\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "\n",
    "            return self\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log failure message in case of an error\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Splitting Data\",\n",
    "                \"status\": \"failed\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            # Save the log\n",
    "            LOAD_LOG(log_msg)\n",
    "\n",
    "    \n",
    "    def train_model(self, n_estimators=100, random_state=42):\n",
    "        \"\"\"\n",
    "        Train the RandomForestRegressor model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_estimators : int, default=100\n",
    "            Number of trees in the random forest\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Train Model\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "\n",
    "            return self\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log failure message in case of an error\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Train Model\",\n",
    "                \"status\": \"failed\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            # Save the log\n",
    "            LOAD_LOG(log_msg)\n",
    "\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate the model using test data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing model evaluation metrics (MSE and R²)\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Evaluate Model\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "\n",
    "            y_pred = self.model.predict(self.X_test)\n",
    "            \n",
    "            mse = mean_squared_error(self.y_test, y_pred)\n",
    "            r2 = r2_score(self.y_test, y_pred)\n",
    "            \n",
    "            metrics = {\n",
    "                'mean_squared_error': mse,\n",
    "                'r2_score': r2\n",
    "            }\n",
    "            \n",
    "            return metrics\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log failure message in case of an error\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Evaluate Model\",\n",
    "                \"status\": \"failed\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            # Save the log\n",
    "            LOAD_LOG(log_msg)\n",
    "    \n",
    "    def tune_hyperparameters(self, param_grid=None):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning using GridSearchCV\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        param_grid : dict, default=None\n",
    "            Grid of parameters for tuning\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Best parameters and best score\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if param_grid is None:\n",
    "                param_grid = {\n",
    "                    'n_estimators': [50, 100, 200],\n",
    "                    'max_depth': [None, 10, 20, 30],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4]\n",
    "                }\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                RandomForestRegressor(random_state=42),\n",
    "                param_grid=param_grid,\n",
    "                cv=5,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            self.model = grid_search.best_estimator_\n",
    "\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Tune Hyperparameter\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "           \n",
    "            \n",
    "            return {\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_score': -grid_search.best_score_  # Negation because scoring is neg_mean_squared_error\n",
    "            }\n",
    "        \n",
    "\n",
    "        except Exception as e:\n",
    "            # Log failure message in case of an error\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Tune Hyperparameter\",\n",
    "                \"status\": \"failed\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            # Save the log\n",
    "            LOAD_LOG(log_msg) \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions with the trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Features for prediction\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        array\n",
    "            Predicted car price values\n",
    "        \"\"\"\n",
    "        try:\n",
    "        # Ensure the given features are consistent with the features used for training\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                # If X is a DataFrame, perform feature engineering\n",
    "                X_copy = X.copy()\n",
    "                X_copy['car_age'] = 2015 - X_copy['year']\n",
    "                X_copy['odometer_log'] = np.log1p(X_copy['odometer'])\n",
    "                \n",
    "                # One-hot encoding\n",
    "                X_encoded = pd.get_dummies(X_copy[self.features], drop_first=True)\n",
    "                \n",
    "                # Ensure all columns used during training are present\n",
    "                missing_cols = set(self.X_encoded.columns) - set(X_encoded.columns)\n",
    "                for col in missing_cols:\n",
    "                    X_encoded[col] = 0\n",
    "                X_encoded = X_encoded[self.X_encoded.columns]\n",
    "                \n",
    "                # Scaling\n",
    "                X_scaled = self.scaler.transform(X_encoded)\n",
    "            else:\n",
    "                # If X is already an array, directly transform\n",
    "                X_scaled = self.scaler.transform(X)\n",
    "\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Predict\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "\n",
    "            return self.model.predict(X_scaled)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log failure message in case of an error\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Predict\",\n",
    "                \"status\": \"failed\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            # Save the log\n",
    "            LOAD_LOG(log_msg) \n",
    "    \n",
    "    def store_model(self, model_filename=\"car_price_model.pkl\", bucket_name=\"car-sales-modelling\", \n",
    "                    minio_host=\"localhost:9001\", minio_access_key=None, minio_secret_key=None):\n",
    "        \"\"\"\n",
    "        Save the machine learning model to MinIO object storage\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_filename : str, default=\"car_price_model.pkl\"\n",
    "            File name for saving the model\n",
    "        bucket_name : str, default=\"car_sales_modelling\"\n",
    "            Bucket name in MinIO\n",
    "        minio_host : str, default=\"localhost:9000\"\n",
    "            MinIO server host and port\n",
    "        minio_access_key : str, default=None\n",
    "            Access key for MinIO authentication (if None, will be taken from environment variables)\n",
    "        minio_secret_key : str, default=None\n",
    "            Secret key for MinIO authentication (if None, will be taken from environment variables)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        bool\n",
    "            True if the model is successfully saved to MinIO, False if failed\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"The model is not trained. Please train the model first with the train_model() method.\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Get credentials from environment variables if not provided\n",
    "            if minio_access_key is None:\n",
    "                minio_access_key = os.getenv('MINIO_ACCESS_KEY')\n",
    "            if minio_secret_key is None:\n",
    "                minio_secret_key = os.getenv('MINIO_SECRET_KEY')\n",
    "                \n",
    "            if minio_access_key is None or minio_secret_key is None:\n",
    "                print(\"Error: MINIO_ACCESS_KEY and MINIO_SECRET_KEY must be provided\")\n",
    "                return False\n",
    "                \n",
    "            # Save model to a local file first\n",
    "            print(f\"Saving model to local file: {model_filename}\")\n",
    "            joblib.dump(self.model, model_filename)\n",
    "            \n",
    "            # Also save the scaler since it's required for prediction\n",
    "            scaler_filename = f\"scaler_{model_filename}\"\n",
    "            joblib.dump(self.scaler, scaler_filename)\n",
    "            \n",
    "            # Save the column information for one-hot encoding\n",
    "            columns_filename = f\"columns_{model_filename}.pkl\"\n",
    "            joblib.dump(self.X_encoded.columns, columns_filename)\n",
    "            \n",
    "            # Initialize MinIO client\n",
    "            print(f\"Connecting to MinIO server: {minio_host}\")\n",
    "            client = Minio(\n",
    "                minio_host,\n",
    "                access_key=minio_access_key,\n",
    "                secret_key=minio_secret_key,\n",
    "                secure=False  # Set to True if using HTTPS\n",
    "            )\n",
    "            \n",
    "            # Check and create the bucket if not exists\n",
    "            if not client.bucket_exists(bucket_name):\n",
    "                print(f\"Bucket '{bucket_name}' not found. Creating new bucket.\")\n",
    "                client.make_bucket(bucket_name)\n",
    "                \n",
    "            # Date for versioning\n",
    "            current_date = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_version_filename = f\"{current_date}_{model_filename}\"\n",
    "            \n",
    "            # Upload model to MinIO\n",
    "            print(f\"Uploading model to bucket '{bucket_name}' with file name '{model_version_filename}'\")\n",
    "            client.fput_object(bucket_name, model_version_filename, model_filename)\n",
    "            \n",
    "            # Upload scaler to MinIO\n",
    "            scaler_version_filename = f\"{current_date}_scaler_{model_filename}\"\n",
    "            client.fput_object(bucket_name, scaler_version_filename, scaler_filename)\n",
    "            \n",
    "            # Upload column information to MinIO\n",
    "            columns_version_filename = f\"{current_date}_columns_{model_filename}\"\n",
    "            client.fput_object(bucket_name, columns_version_filename, columns_filename)\n",
    "            \n",
    "            # Remove local files after upload\n",
    "            os.remove(model_filename)\n",
    "            os.remove(scaler_filename)\n",
    "            os.remove(columns_filename)\n",
    "            \n",
    "            print(\"Model, scaler, and column information successfully saved to MinIO.\")\n",
    "\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Store Model\",\n",
    "                \"status\": \"success!\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "\n",
    "            return True\n",
    "        \n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log failure message in case of an error\n",
    "            log_msg = {\n",
    "                \"step\": \"modelling\",\n",
    "                \"component\": \"Predict\",\n",
    "                \"status\": \"failed\",\n",
    "                \"table_name\": \"car_price\",\n",
    "                \"etl_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error_msg\": str(e)\n",
    "            }\n",
    "\n",
    "            print(f\"Error saving model to MinIO: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            # Save the log\n",
    "            LOAD_LOG(log_msg) \n",
    "    \n",
    "    def run_pipeline(self, tune=False, store=False):\n",
    "        \"\"\"\n",
    "        Run the entire modeling pipeline from start to finish\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        tune : bool, default=False\n",
    "            Whether to perform hyperparameter tuning\n",
    "        store : bool, default=False\n",
    "            Whether to save the model to MinIO after training\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Model evaluation metrics\n",
    "        \"\"\"\n",
    "        self.feature_engineering()\n",
    "        self.prepare_data()\n",
    "        self.scale_features()\n",
    "        self.split_data()\n",
    "        \n",
    "        if tune:\n",
    "            tuning_results = self.tune_hyperparameters()\n",
    "            print(f\"Best parameters: {tuning_results['best_params']}\")\n",
    "            print(f\"Best MSE score: {tuning_results['best_score']}\")\n",
    "        else:\n",
    "            self.train_model()\n",
    "        \n",
    "        metrics = self.evaluate_model()\n",
    "        print(f\"Mean Squared Error: {metrics['mean_squared_error']}\")\n",
    "        print(f\"R-squared: {metrics['r2_score']}\")\n",
    "        \n",
    "        if store:\n",
    "            self.store_model()\n",
    "        \n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = CarPriceModel(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2710093.717584965\n",
      "R-squared: 0.9718879951976879\n",
      "Saving model to local file: car_price_model.pkl\n",
      "Connecting to MinIO server: localhost:9001\n",
      "Bucket 'car-sales-modelling' not found. Creating new bucket.\n",
      "Uploading model to bucket 'car-sales-modelling' with file name '20250424_205128_car_price_model.pkl'\n",
      "Model, scaler, and column information successfully saved to MinIO.\n"
     ]
    }
   ],
   "source": [
    "metrics = df_model.run_pipeline(store=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
